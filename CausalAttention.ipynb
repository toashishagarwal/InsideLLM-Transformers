{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeRd6Wufyg8c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Causal Self-Attention module where each position can only attend to previous positions.\n",
        "\n",
        "    This is used in decoder-only architectures like GPT, where the model should\n",
        "    not see future tokens during training or inference.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the causal self-attention module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): The dimension of the input embeddings\n",
        "            dropout (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.scale = math.sqrt(embedding_dim)\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for causal self-attention\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            output: Causal self-attention output (batch_size, seq_len, embedding_dim)\n",
        "            attention: Attention weights (batch_size, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.query(x)  # (batch_size, seq_len, embedding_dim)\n",
        "        k = self.key(x)    # (batch_size, seq_len, embedding_dim)\n",
        "        v = self.value(x)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, seq_len, embedding_dim) @ (batch_size, embedding_dim, seq_len)\n",
        "        # -> (batch_size, seq_len, seq_len)\n",
        "        scores = torch.bmm(q, k.transpose(1, 2)) / self.scale\n",
        "\n",
        "        # Create causal mask (lower triangular) to ensure each position only attends to previous positions\n",
        "        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n",
        "        scores = scores.masked_fill(causal_mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, embedding_dim)\n",
        "        # -> (batch_size, seq_len, embedding_dim)\n",
        "        output = torch.bmm(attention, v)\n",
        "\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "vdDDZ2tYzOBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def test_causal_attention():\n",
        "    batch_size = 1        #4\n",
        "    seq_len = 4           #10\n",
        "    embedding_dim = 8     #64\n",
        "\n",
        "    # Create random input tensor\n",
        "    x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "    # Initialize causal-attention modules\n",
        "    causal_attn = CausalSelfAttention(embedding_dim)\n",
        "\n",
        "    # Forward passes\n",
        "    output, attention = causal_attn(x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(\"Our Input tensor is --> \")\n",
        "    print(x)\n",
        "\n",
        "    print(f\"Causal-attention output shape: {output.shape}\")\n",
        "    print(\"Our Output Context matrix  is --> \")\n",
        "    print(output)\n",
        "\n",
        "    print(f\"Causal-attention weights shape: {attention.shape}\")\n",
        "    print(\"Our Output Attention matrix  is --> \")\n",
        "    print(attention)\n",
        "\n",
        "    # Verify that causal attention has the right pattern (lower triangular)\n",
        "    print(f\"Is causal mask working? {torch.all(torch.triu(attention[0], diagonal=1) == 0)}\")\n",
        "\n",
        "    return output, attention\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_causal_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcMCcv3rzVni",
        "outputId": "b4a81813-10ea-44ef-a9e6-6690f39c7ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 4, 8])\n",
            "Our Input tensor is --> \n",
            "tensor([[[ 1.3303, -1.1235,  1.1652, -0.0349,  0.1052, -1.6020,  0.6992,\n",
            "           1.3815],\n",
            "         [ 1.3089,  1.1137, -1.1362, -0.9922,  0.0993,  0.5333, -1.9262,\n",
            "           1.2088],\n",
            "         [-0.3219, -0.3680,  0.4476,  1.2465, -0.4465,  1.4749,  0.4591,\n",
            "           1.4252],\n",
            "         [-0.5158, -0.5374,  0.1797,  1.6236, -0.9445, -0.8403,  0.2905,\n",
            "           1.3741]]])\n",
            "Causal-attention output shape: torch.Size([1, 4, 8])\n",
            "Our Output Context matrix  is --> \n",
            "tensor([[[ 0.2434,  0.3784, -0.5413, -0.1418,  1.0619, -0.0691,  0.6464,\n",
            "          -0.6711],\n",
            "         [ 0.0873,  0.1358, -0.1943, -0.0509,  0.3811, -0.0248,  0.2320,\n",
            "          -0.2408],\n",
            "         [-0.3893, -0.2056, -0.4329,  0.4260,  0.0843, -0.1285,  0.2085,\n",
            "          -0.1206],\n",
            "         [-0.0736, -0.3307, -0.5131,  0.8113,  0.5907,  0.1014,  0.6621,\n",
            "           0.2256]]], grad_fn=<BmmBackward0>)\n",
            "Causal-attention weights shape: torch.Size([1, 4, 4])\n",
            "Our Output Attention matrix  is --> \n",
            "tensor([[[1.1111, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3987, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.3753, 0.0000, 0.0000],\n",
            "         [0.0000, 0.3034, 0.2294, 0.3280]]], grad_fn=<MulBackward0>)\n",
            "Is causal mask working? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afMVS25PzRgd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}