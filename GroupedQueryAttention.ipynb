{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "xVlgEiYTd8xo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YICODUnwd3E9"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention (GQA) implementation.\n",
        "\n",
        "    GQA is an optimization technique that reduces computational complexity\n",
        "    by using fewer query heads than key/value heads while maintaining performance.\n",
        "    This allows for more efficient inference in large language models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_query_heads, num_kv_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the Grouped Query Attention module.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimension of the model (embedding dimension)\n",
        "            num_query_heads (int): Number of query attention heads\n",
        "            num_kv_heads (int): Number of key/value attention heads\n",
        "            dropout (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super(GroupedQueryAttention, self).__init__()\n",
        "\n",
        "        # Ensure d_model is divisible by both the number of query and kv heads\n",
        "        assert d_model % num_query_heads == 0, \"d_model must be divisible by num_query_heads\"\n",
        "        assert d_model % num_kv_heads == 0, \"d_model must be divisible by num_kv_heads\"\n",
        "        # Ensure num_query_heads is a multiple of num_kv_heads\n",
        "        assert num_query_heads % num_kv_heads == 0, \"num_query_heads must be a multiple of num_kv_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_query_heads = num_query_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.kv_groups = num_query_heads // num_kv_heads  # How many query heads share a single kv head\n",
        "\n",
        "        self.d_qk = d_model // num_query_heads  # Dimension per query head\n",
        "        self.d_v = d_model // num_kv_heads      # Dimension per value head\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.query_proj = nn.Linear(d_model, d_model)\n",
        "        self.key_proj = nn.Linear(d_model, (d_model // num_query_heads) * num_kv_heads)\n",
        "        self.value_proj = nn.Linear(d_model, (d_model // num_query_heads) * num_kv_heads)\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for Grouped Query Attention\n",
        "\n",
        "        Args:\n",
        "            query: Query tensor (batch_size, seq_len_q, d_model)\n",
        "            key: Key tensor (batch_size, seq_len_k, d_model)\n",
        "            value: Value tensor (batch_size, seq_len_v, d_model)\n",
        "            mask: Optional mask tensor for masked attention\n",
        "\n",
        "        Returns:\n",
        "            output: Attention output (batch_size, seq_len_q, d_model)\n",
        "            attention_weights: Attention weights\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "        seq_len_q = query.size(1)\n",
        "        seq_len_k = key.size(1)\n",
        "\n",
        "        # Linear projections\n",
        "        # Q: (batch_size, seq_len_q, d_model)\n",
        "        q = self.query_proj(query)\n",
        "        # K, V: (batch_size, seq_len_k, (d_model // num_query_heads) * num_kv_heads)\n",
        "        k = self.key_proj(key)\n",
        "        v = self.value_proj(value)\n",
        "\n",
        "        # Reshape for attention computation\n",
        "        # q: (batch_size, num_query_heads, seq_len_q, d_qk)\n",
        "        q = q.view(batch_size, seq_len_q, self.num_query_heads, self.d_qk).transpose(1, 2)\n",
        "        # k, v: (batch_size, num_kv_heads, seq_len_k, d_qk)\n",
        "        k = k.view(batch_size, seq_len_k, self.num_kv_heads, self.d_qk).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len_k, self.num_kv_heads, self.d_qk).transpose(1, 2)\n",
        "\n",
        "        # Expand K, V to match the number of query heads\n",
        "        # This implements the \"grouping\" where multiple query heads share the same key-value head\n",
        "        # k, v: (batch_size, num_query_heads, seq_len_k, d_qk)\n",
        "        k = torch.repeat_interleave(k, self.kv_groups, dim=1)\n",
        "        v = torch.repeat_interleave(v, self.kv_groups, dim=1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, num_query_heads, seq_len_q, seq_len_k)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_qk)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        # (batch_size, num_query_heads, seq_len_q, d_qk)\n",
        "        context = torch.matmul(attention_weights, v)\n",
        "\n",
        "        # Reshape back to original dimensions\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.output_proj(context)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage and testing\n",
        "def test_grouped_query_attention():\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "    d_model = 512\n",
        "    num_query_heads = 8\n",
        "    num_kv_heads = 2  # Each KV head is shared across 4 query heads\n",
        "\n",
        "    # Create random input tensors\n",
        "    query = torch.randn(batch_size, seq_len, d_model)\n",
        "    key = torch.randn(batch_size, seq_len, d_model)\n",
        "    value = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create mask (for causal/self-attention)\n",
        "    mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).unsqueeze(0).unsqueeze(0)\n",
        "    mask = (1.0 - mask).bool()  # Convert to boolean mask where 1 means keep, 0 means mask\n",
        "\n",
        "    # Initialize GQA layer\n",
        "    gqa = GroupedQueryAttention(d_model, num_query_heads, num_kv_heads)\n",
        "\n",
        "    # Forward pass\n",
        "    output, attention = gqa(query, key, value, mask)\n",
        "\n",
        "    print(f\"Input shape: {query.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {attention.shape}\")\n",
        "    print(f\"Number of query heads: {num_query_heads}\")\n",
        "    print(f\"Number of key/value heads: {num_kv_heads}\")\n",
        "    print(f\"Grouping factor: {num_query_heads // num_kv_heads}\")\n",
        "\n",
        "    # Check if output maintains input dimensions\n",
        "    assert output.shape == query.shape, \"Output shape should match input shape\"\n",
        "\n",
        "    return output, attention\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_grouped_query_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CyNO-kveCl9",
        "outputId": "c1ac4b1c-c561-46b7-b1e1-1c436cdc8146"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 10, 512])\n",
            "Output shape: torch.Size([2, 10, 512])\n",
            "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
            "Number of query heads: 8\n",
            "Number of key/value heads: 2\n",
            "Grouping factor: 4\n"
          ]
        }
      ]
    }
  ]
}