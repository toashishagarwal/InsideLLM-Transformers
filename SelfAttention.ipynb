{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s62YgL2Tq5kX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention module where a sequence attends to itself.\n",
        "\n",
        "    This is a fundamental building block in transformer-based architectures\n",
        "    where each position in a sequence can attend to all positions in the same sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the self-attention module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): The dimension of the input embeddings\n",
        "            dropout (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.scale = math.sqrt(embedding_dim)\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for self-attention\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_len, embedding_dim)\n",
        "            mask: Optional mask tensor for masking out certain positions\n",
        "\n",
        "        Returns:\n",
        "            output: Self-attention output (batch_size, seq_len, embedding_dim)\n",
        "            attention: Attention weights (batch_size, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.query(x)  # (batch_size, seq_len, embedding_dim)\n",
        "        k = self.key(x)    # (batch_size, seq_len, embedding_dim)\n",
        "        v = self.value(x)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, seq_len, embedding_dim) @ (batch_size, embedding_dim, seq_len)\n",
        "        # -> (batch_size, seq_len, seq_len)\n",
        "        scores = torch.bmm(q, k.transpose(1, 2)) / self.scale\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, embedding_dim)\n",
        "        # -> (batch_size, seq_len, embedding_dim)\n",
        "        output = torch.bmm(attention, v)\n",
        "\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "MhREVZzoruxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def test_self_attention():\n",
        "    batch_size = 4\n",
        "    seq_len = 10\n",
        "    embedding_dim = 64\n",
        "\n",
        "    # Create random input tensor\n",
        "    x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "\n",
        "    # Initialize self-attention modules\n",
        "    self_attn = SelfAttention(embedding_dim)\n",
        "\n",
        "    # Forward passes\n",
        "    output1, attention1 = self_attn(x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(\"Our Input tensor is --> \")\n",
        "    print(x)\n",
        "\n",
        "    print(f\"Self-attention output shape: {output1.shape}\")\n",
        "    print(\"Our Output Context matrix  is --> \")\n",
        "    print(output1)\n",
        "\n",
        "    print(f\"Self-attention weights shape: {attention1.shape}\")\n",
        "    print(\"Our Output Attention matrix  is --> \")\n",
        "    print(attention1)\n",
        "\n",
        "    return output1, attention1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_self_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlSIAEzQr0WX",
        "outputId": "5a73666b-46ea-45d9-e708-9c27052ebc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10, 64])\n",
            "Our Input tensor is --> \n",
            "tensor([[[-0.8841,  0.0142, -0.0912,  ..., -0.6959,  0.0986,  0.9691],\n",
            "         [-0.0103,  0.9237,  1.1994,  ..., -0.0157,  0.2500,  0.7310],\n",
            "         [ 0.6077,  0.2446,  1.0081,  ..., -0.4376,  1.1936,  0.2615],\n",
            "         ...,\n",
            "         [ 1.0289,  0.9542,  0.3406,  ..., -2.0422, -0.6019,  0.2761],\n",
            "         [ 1.6400,  0.4933, -0.8890,  ...,  1.2761,  0.8840,  1.1745],\n",
            "         [-0.4358, -0.3111,  1.1548,  ...,  0.5248,  1.1690, -1.0264]],\n",
            "\n",
            "        [[-0.7454, -1.2645,  0.9789,  ..., -0.9478, -0.0955, -0.0098],\n",
            "         [-1.0414, -0.0137, -0.7522,  ...,  0.4737,  1.6196,  0.1940],\n",
            "         [ 1.7506,  2.2319,  0.9934,  ..., -1.7916,  0.7520, -0.6998],\n",
            "         ...,\n",
            "         [ 0.5531, -0.2466,  0.2250,  ..., -0.1500, -1.5713, -0.0282],\n",
            "         [-0.2143, -0.0576, -0.1151,  ..., -1.7637,  0.2469,  0.1862],\n",
            "         [ 0.4525, -1.0666, -1.6398,  ...,  0.1193, -1.0125, -1.2960]],\n",
            "\n",
            "        [[-0.7578,  0.5578, -0.7382,  ...,  1.3067, -0.9049,  0.4548],\n",
            "         [ 0.1399, -1.5314, -0.0156,  ..., -1.8940,  0.3963,  1.3894],\n",
            "         [ 0.1881,  1.4606, -1.3229,  ...,  1.0185,  1.6156,  1.8217],\n",
            "         ...,\n",
            "         [-1.2240, -0.6462,  1.2435,  ..., -0.8918, -0.1147,  2.5680],\n",
            "         [-0.4547,  1.7643,  0.0487,  ...,  1.7850, -1.5527,  0.0234],\n",
            "         [ 0.6633, -0.2531,  1.0872,  ..., -1.0296, -0.8575,  1.5916]],\n",
            "\n",
            "        [[ 0.2895,  1.4642, -1.2773,  ...,  0.3524,  1.1825, -0.8472],\n",
            "         [-0.9918,  1.4135,  0.3526,  ...,  1.3305,  0.5794, -0.5473],\n",
            "         [ 0.6800, -0.8568, -0.1668,  ..., -0.5181, -0.6115,  2.0650],\n",
            "         ...,\n",
            "         [-1.2070,  1.1127,  0.3954,  ...,  0.3722, -0.2048,  1.3021],\n",
            "         [-0.7704,  1.2625, -0.1314,  ..., -1.6780,  0.2830, -0.1030],\n",
            "         [ 1.1143,  0.5713,  0.4590,  ...,  0.9075,  0.7323, -0.3752]]])\n",
            "Self-attention output shape: torch.Size([4, 10, 64])\n",
            "Our Output Context matrix  is --> \n",
            "tensor([[[-0.3245, -0.2495, -0.4340,  ..., -0.0301,  0.2897, -0.0950],\n",
            "         [-0.0056, -0.0795, -0.2010,  ...,  0.0108,  0.3439, -0.0349],\n",
            "         [-0.1115, -0.2715, -0.2782,  ...,  0.1700,  0.2774, -0.1340],\n",
            "         ...,\n",
            "         [-0.0056, -0.1954, -0.1077,  ...,  0.1503,  0.3277, -0.1164],\n",
            "         [-0.2559, -0.1235, -0.3792,  ...,  0.1117,  0.2135, -0.1062],\n",
            "         [-0.0540, -0.0967, -0.2986,  ...,  0.1453,  0.3577, -0.0243]],\n",
            "\n",
            "        [[ 0.2916,  0.0630,  0.0887,  ...,  0.1636,  0.0043, -0.0368],\n",
            "         [ 0.1732,  0.0126,  0.1108,  ...,  0.1250,  0.0503, -0.0283],\n",
            "         [ 0.3685,  0.0062,  0.0074,  ...,  0.0961, -0.0601, -0.0575],\n",
            "         ...,\n",
            "         [ 0.3167,  0.0575,  0.1012,  ...,  0.1443, -0.0461, -0.0430],\n",
            "         [ 0.2325,  0.1183,  0.0814,  ...,  0.0139, -0.1937, -0.0189],\n",
            "         [ 0.1051, -0.0213, -0.0229,  ..., -0.0515, -0.0598, -0.0377]],\n",
            "\n",
            "        [[ 0.1194, -0.1828, -0.0573,  ..., -0.3331, -0.0478,  0.1007],\n",
            "         [ 0.0913, -0.2164, -0.0129,  ..., -0.3123, -0.0523,  0.2087],\n",
            "         [ 0.0147, -0.0318, -0.0867,  ..., -0.2845, -0.0745,  0.1656],\n",
            "         ...,\n",
            "         [-0.0208, -0.0139,  0.0320,  ..., -0.2944,  0.0039,  0.2101],\n",
            "         [-0.0293,  0.0395,  0.0022,  ..., -0.2158, -0.0513,  0.1619],\n",
            "         [ 0.0007,  0.1289,  0.1397,  ..., -0.2333,  0.0090,  0.0688]],\n",
            "\n",
            "        [[-0.1302,  0.2380, -0.1584,  ..., -0.0053, -0.1670, -0.2212],\n",
            "         [-0.3298,  0.1783, -0.1830,  ...,  0.0048, -0.0184, -0.0291],\n",
            "         [-0.1489,  0.1376, -0.2395,  ...,  0.2754, -0.1260, -0.1948],\n",
            "         ...,\n",
            "         [-0.0033,  0.3483, -0.1309,  ..., -0.0720, -0.2140, -0.0200],\n",
            "         [ 0.0527,  0.3529, -0.1791,  ...,  0.2504, -0.2435, -0.2641],\n",
            "         [-0.1105,  0.3092, -0.2001,  ...,  0.2139, -0.1666, -0.0921]]],\n",
            "       grad_fn=<BmmBackward0>)\n",
            "Self-attention weights shape: torch.Size([4, 10, 10])\n",
            "Our Output Attention matrix  is --> \n",
            "tensor([[[0.0761, 0.1117, 0.0000, 0.1326, 0.1016, 0.1080, 0.0664, 0.1248,\n",
            "          0.1720, 0.1148],\n",
            "         [0.1406, 0.1252, 0.0000, 0.0000, 0.0626, 0.0000, 0.1293, 0.1302,\n",
            "          0.1610, 0.0000],\n",
            "         [0.1148, 0.1089, 0.1077, 0.1065, 0.1019, 0.1776, 0.0780, 0.0731,\n",
            "          0.1271, 0.1154],\n",
            "         [0.0000, 0.0937, 0.1044, 0.0762, 0.1487, 0.1688, 0.1204, 0.0680,\n",
            "          0.1150, 0.1205],\n",
            "         [0.0000, 0.1106, 0.0992, 0.2107, 0.0997, 0.0911, 0.1274, 0.0974,\n",
            "          0.1026, 0.0655],\n",
            "         [0.0734, 0.1225, 0.0000, 0.0765, 0.0000, 0.1558, 0.0985, 0.1499,\n",
            "          0.1031, 0.0743],\n",
            "         [0.1007, 0.1330, 0.1086, 0.0872, 0.1609, 0.1022, 0.0718, 0.1238,\n",
            "          0.0671, 0.1558],\n",
            "         [0.1770, 0.0975, 0.1330, 0.0891, 0.1437, 0.1176, 0.0802, 0.0818,\n",
            "          0.0721, 0.1191],\n",
            "         [0.1155, 0.0000, 0.0419, 0.0785, 0.1272, 0.1438, 0.0797, 0.1053,\n",
            "          0.1289, 0.1335],\n",
            "         [0.1398, 0.1014, 0.0964, 0.0000, 0.1172, 0.0682, 0.0705, 0.1187,\n",
            "          0.1521, 0.1468]],\n",
            "\n",
            "        [[0.1256, 0.0850, 0.0983, 0.1314, 0.1187, 0.1209, 0.1477, 0.0939,\n",
            "          0.0000, 0.1184],\n",
            "         [0.0961, 0.1238, 0.0657, 0.1001, 0.1097, 0.1142, 0.1163, 0.1463,\n",
            "          0.0000, 0.1184],\n",
            "         [0.0966, 0.0000, 0.0563, 0.1262, 0.1265, 0.0000, 0.1014, 0.1574,\n",
            "          0.0692, 0.0000],\n",
            "         [0.1724, 0.1141, 0.0729, 0.1496, 0.1210, 0.0682, 0.1339, 0.0980,\n",
            "          0.0932, 0.0877],\n",
            "         [0.1506, 0.1534, 0.0973, 0.0841, 0.1382, 0.1236, 0.0876, 0.1018,\n",
            "          0.0891, 0.0855],\n",
            "         [0.0908, 0.0848, 0.0890, 0.0654, 0.1510, 0.1195, 0.1380, 0.1228,\n",
            "          0.1371, 0.1127],\n",
            "         [0.1133, 0.0939, 0.1443, 0.1097, 0.1643, 0.0761, 0.0000, 0.1005,\n",
            "          0.0756, 0.0000],\n",
            "         [0.0992, 0.1463, 0.1573, 0.1141, 0.0967, 0.1107, 0.1351, 0.0587,\n",
            "          0.0000, 0.0983],\n",
            "         [0.1230, 0.1160, 0.1417, 0.0000, 0.1811, 0.0000, 0.0000, 0.0872,\n",
            "          0.1231, 0.1080],\n",
            "         [0.0693, 0.0932, 0.0404, 0.0000, 0.1178, 0.0968, 0.0794, 0.1447,\n",
            "          0.1234, 0.0000]],\n",
            "\n",
            "        [[0.1424, 0.0718, 0.1857, 0.0551, 0.0754, 0.1504, 0.0877, 0.1117,\n",
            "          0.1023, 0.1285],\n",
            "         [0.1337, 0.0970, 0.1392, 0.0381, 0.1495, 0.1488, 0.1150, 0.0736,\n",
            "          0.0631, 0.1533],\n",
            "         [0.1212, 0.1347, 0.1379, 0.1086, 0.0751, 0.1398, 0.1040, 0.0762,\n",
            "          0.1340, 0.0795],\n",
            "         [0.1015, 0.0664, 0.0984, 0.1173, 0.1395, 0.2137, 0.0729, 0.1286,\n",
            "          0.0852, 0.0877],\n",
            "         [0.1237, 0.1377, 0.1114, 0.1539, 0.0983, 0.1445, 0.0884, 0.0861,\n",
            "          0.0937, 0.0734],\n",
            "         [0.1012, 0.0735, 0.2188, 0.1104, 0.0000, 0.1202, 0.0552, 0.1390,\n",
            "          0.1324, 0.0788],\n",
            "         [0.0000, 0.0741, 0.0857, 0.1102, 0.1694, 0.1762, 0.0789, 0.1128,\n",
            "          0.1797, 0.0513],\n",
            "         [0.1296, 0.1123, 0.1173, 0.1078, 0.1333, 0.0989, 0.1212, 0.1235,\n",
            "          0.0834, 0.0838],\n",
            "         [0.1346, 0.1128, 0.0808, 0.1022, 0.1066, 0.1513, 0.0988, 0.0755,\n",
            "          0.1761, 0.0723],\n",
            "         [0.0879, 0.0000, 0.1103, 0.1117, 0.1263, 0.0827, 0.1179, 0.1563,\n",
            "          0.1393, 0.0814]],\n",
            "\n",
            "        [[0.0781, 0.0000, 0.1182, 0.1136, 0.0754, 0.1040, 0.0675, 0.1013,\n",
            "          0.0850, 0.1709],\n",
            "         [0.0569, 0.1273, 0.0998, 0.0904, 0.1100, 0.0702, 0.0000, 0.1430,\n",
            "          0.0706, 0.2299],\n",
            "         [0.0994, 0.0925, 0.2126, 0.0742, 0.1609, 0.0885, 0.0666, 0.1050,\n",
            "          0.0731, 0.1384],\n",
            "         [0.1249, 0.0000, 0.1040, 0.0779, 0.0671, 0.0761, 0.2194, 0.0969,\n",
            "          0.1113, 0.1149],\n",
            "         [0.1080, 0.0916, 0.0830, 0.0879, 0.1189, 0.1211, 0.0657, 0.1588,\n",
            "          0.1576, 0.1185],\n",
            "         [0.1241, 0.0762, 0.0941, 0.1616, 0.1292, 0.1186, 0.0825, 0.1122,\n",
            "          0.1223, 0.0904],\n",
            "         [0.0419, 0.0675, 0.1576, 0.2018, 0.1783, 0.1034, 0.1232, 0.0000,\n",
            "          0.0788, 0.1025],\n",
            "         [0.0906, 0.0000, 0.0628, 0.0820, 0.0000, 0.1508, 0.0000, 0.0963,\n",
            "          0.1472, 0.1154],\n",
            "         [0.1182, 0.0962, 0.0743, 0.0578, 0.0000, 0.1308, 0.2340, 0.1011,\n",
            "          0.1599, 0.0761],\n",
            "         [0.1326, 0.1384, 0.0826, 0.0701, 0.0743, 0.1159, 0.0975, 0.1442,\n",
            "          0.1378, 0.1177]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    }
  ]
}